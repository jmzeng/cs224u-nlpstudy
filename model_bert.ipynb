{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learing to write out the BERT architecture\n",
    "# Embeddings\n",
    "# Positional Embedding / Encoding\n",
    "# Encoder\n",
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostionalEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PostionalEmbeddings, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "                pe[pos, i+1] = math.cos(pos / (10000 ** (2 * (i + 1) / d_model)))\n",
    "\n",
    "        pe = pe.unsqueeze(0) # add batch size dimension\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_seq_len, dropout):\n",
    "        super(BERTEmbeddings, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        # Token embeddings, this is the normal text embedding\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        # Segment embeddings, adding sentence segment info\n",
    "        self.segment_embed = nn.Embedding(3, embed_size, padding_idx=0)\n",
    "        # Positional embeddings\n",
    "        self.position = PostionalEmbeddings(embed_size, max_seq_len)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input, segment_label):\n",
    "        x = self.position(self.token_embed(input)) + self.segment_embed(segment_label)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init()\n",
    "        \n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        # new dim: [batch_size, self.n_heads, seq_length, self.d_k]\n",
    "        return x.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def attention(self, Q, K, V, mask):\n",
    "        # [batch_size, self.n_heads, seq_length, self.d_k] * [batch_size, self.n_heads, self.d_k, seq_length]\n",
    "        # [batch_size, self.n_heads, seq_length, seq_length]\n",
    "        attn_score = torch.matmal(Q, K.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Add low values for masked items\n",
    "        attn_score = torch.masked_fill(mask == 0, -1e-9)\n",
    "\n",
    "        # [batch_size, self.n_heads, seq_length, seq_length]\n",
    "        attn_probs = torch.softmax(attn_score, dim=-1)\n",
    "\n",
    "        # [batch_size, self.n_heads, seq_length, self.d_k]\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        # x dimensions is [batch_size, seq_length, d_model]\n",
    "        # Initialize matrices and split heads\n",
    "        Q = self.split_heads(self.W_q(Q)) # [batch_size, seq_length, d_model]\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        # Apply attention mechanism to head\n",
    "        # [batch_size, self.n_heads, seq_length, self.d_k]\n",
    "        attn_output = self.attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine heads\n",
    "        # Reshape back to [batch_size, seq_length, d_model]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        return self.W_o(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.fc1(x))\n",
    "        return self.fc2(self.dropout(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_heads, dropout):\n",
    "        super().__init()\n",
    "        self.multihead = MultiHeadAttention(d_model, n_heads)\n",
    "        self.feedforward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Embeddings: [batch_size, seq_length, d_model]\n",
    "        # Mask: [batch_size, 1, 1, seq_length]\n",
    "        # Ouput: [batch_size, seq_length, d_model]\n",
    "        attn_output = self.multihead(x, x, x, mask)\n",
    "        out = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feedforward(out)\n",
    "        out = self.norm2(x + self.dropout(ff_output))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTPooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init()\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, encoder_out):\n",
    "        cls_token = encoder_out[:, 0]\n",
    "        out = self.tanh(self.dense(cls_token))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_length, d_model, n_heads, d_ff, n_layers, dropout):\n",
    "        super().__init()\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embeddings = BERTEmbeddings(vocab_size, d_model, seq_length, dropout)\n",
    "        self.encoder = nn.ModuleList([EncodeLayer(d_model, d_ff, n_heads, dropout) for _ in range(n_layers)])\n",
    "        \n",
    "        # mlm\n",
    "        self.mlm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # next sentence prediction\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.nsp_head = nn.Linear(d_model, 2) \n",
    "\n",
    "    def forward(self, input, segment_label, mask):\n",
    "        x = self.embeddings(input, segment_label)\n",
    "        \n",
    "        out = embeds\n",
    "        # [batch_size, seq_length, d_model]\n",
    "        for layer in self.encoder:\n",
    "            out = layer(out, mask)\n",
    "\n",
    "        mlm_output = self.mlm_head(out) # [batch_size, seq_length, vocab_size]\n",
    "        \n",
    "        cls_token = out[:, 0] # Shape: [batch_size, d_model]\n",
    "        nsp_output = self.nsp_head(cls_token) # Shape: [batch_size, 2]\n",
    "\n",
    "        return self.softmax(mlm_output), self.softmax(nsp_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassification(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_length, d_model, n_heads, n_classes, d_ff, n_layers, dropout):\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embeddings = BERTEmbeddings(vocab_size, d_model, seq_length, dropout)\n",
    "        self.encoder = nn.ModuleList([EncodeLayer(d_model, d_ff, n_heads, dropout) for _ in range(n_layers)])\n",
    "        self.pooler = BERTPooler(d_model)\n",
    "\n",
    "        self.classifier = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, input, segment_label, mask):\n",
    "        x = self.embeddings(input, segment_label)\n",
    "        \n",
    "        out = embeds\n",
    "        # [batch_size, seq_length, d_model]\n",
    "        for layer in self.encoder:\n",
    "            out = layer(out, mask)\n",
    "        \n",
    "        pooled_output = self.pooler(out)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT(vocab_size=2000, seq_len = 512, d_model=768, n_heads=3, n_layers=12, d_ff=4*768)\n",
    "\n",
    "dataloader\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "criterion = nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for i, (input, is_next, bert_label, segment) in enumerate(data_loader):\n",
    "        mlm_output, nsp_output = model(input, segment, mask)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # NSP Loss\n",
    "        nsp_loss = criterion(nsp_output, is_next)\n",
    "\n",
    "        # MLM loss\n",
    "        mlm_loss = criterion(mlm_output.transpose(1, 2), bert_label)\n",
    "        loss = nsp_loss + mlm_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
