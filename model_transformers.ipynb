{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Coding up a transformers architecture\n",
    "# Embedding layer\n",
    "# Positional encoding (done)\n",
    "# Multi-head attention (done)\n",
    "# Feedforward network (done)\n",
    "# Residual connections\n",
    "# Layer normalization\n",
    "# Encoder\n",
    "# Decoder\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, drop_prob=0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "\n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "\n",
    "        # Transposing gives dimension [batch_size, n_heads, seq_length, d_k]\n",
    "        return x.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def attention(self, Q, K, V, mask=None):\n",
    "        # Calcualte attention scores\n",
    "        # [batch_size, n_heads, seq_length, d_k] * [batch_size, n_heads, d_k, seq_length]\n",
    "        # =  [batch_size, n_heads, seq_length, seq_length]\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "        # [batch_size, num_heads, seq_length, seq_length] \n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        attn_probs = self.dropout(attn_probs) # Dropout can be applied in a number of places\n",
    "\n",
    "        # [batch_size, num_heads, seq_length, seq_length] * [batch_size, n_heads, seq_length, d_k]\n",
    "        # = [batch_size, n_heads, seq_length, d_k]\n",
    "        output = torch.matmal(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, n_heads, seq_length, d_k = x.size()\n",
    "\n",
    "        # Recombine tensor to shape [batch_size, seq_length, d_model]\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "\n",
    "        # Apply linear transformation [batch_size, seq_length, d_model]\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "\n",
    "        # Split heads [batch_size, n_heads, seq_length, d_k]\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "\n",
    "        # Perform attention on each\n",
    "        attn_output = self.attention(Q, K, V, mask)\n",
    "\n",
    "        # Recombine heads\n",
    "        attn_concat = self.combine_heads(attn_output)\n",
    "        \n",
    "        # Output transformation\n",
    "        output = self.W_o(attn_concat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(max_seq_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # [1, max_seq_length, d_model]\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Register as non-learnable parameter\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Ensure the positional encodings match the actual sequence of x\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(n_heads, d_model)\n",
    "        self.feedforward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(n_heads, d_model)\n",
    "        self.cross_attn = MultiHeadAttention(n_heads, d_model)\n",
    "        self.feedforward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, n_heads, n_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(self, n_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        src_embeddings = self.encoder_embedding(src)\n",
    "        src_embeddings = self.dropout(self.positional_encoding(src_embeddings))\n",
    "\n",
    "        tgt_embeddings = self.decoder_embedding(tgt)\n",
    "        tgt_embeddings = self.dropout(self.positional_encoding(tgt_embeddings))\n",
    "\n",
    "        enc_output = src_embeddings\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "        \n",
    "        dec_output = dec_embeddings\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.fc(dec_outputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(100, 3)\n",
    "\n",
    "y = x * np.sin(x) + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(ExampleDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ExampleDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
